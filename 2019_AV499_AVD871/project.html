<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="lectures.html">Lectures</a></div>
<div class="menu-item"><a href="project.html">Project</a></div>
<div class="menu-item"><a href="classtest_exams.html">Class&nbsp;tests&nbsp;&amp;&nbsp;Exams</a></div>
</td>
<td id="layout-content">
<h1>Applied Markov decision processes &amp; Reinforcement Learning (AV499 &amp; AVD871)</h1>
<h2>Points to note</h2>
<div class="infoblock">
<div class="blockcontent">
<ul>
<li><p>The class project is a team project for students who are crediting the course</p>
</li>
<li><p>The team size should be 3</p>
</li>
<li><p>The class project carries 20 marks and will be averaged with the internal marks (also of 20 marks)</p>
</li>
<li><p>The project can be of the following types</p>
<ul>
<li><p>Picking a research paper in Markov decision processes and Reinforcement learning from the options given below (see below for how to choose a paper), writing a report on the paper. and giving a presentation on the main ideas in the paper. Implementation and reproduction of the results in the paper would carry additional credit.</p>
</li>
<li><p>Picking a software framework or library which is used for Markov decision processes and Reinforcement learning, writing a tutorial article on this with a novel example, giving a tutorial presentation on the use of the software. See a list of options below.</p>
</li>
<li><p>Proposing your own research idea or implementation project in the field of stochastic control (MDP) and Reinforcement learning, writing up a research proposal,and writing up a research paper or making a demonstration on this idea (please note that there is no extra credit for this and this is highly risky as far as marks are concerned). Please discuss with the instructor.</p>
</li>
</ul>

</li>
</ul>
</div></div>
<h2>Research paper options</h2>
<div class="infoblock">
<div class="blockcontent">
<ul>
<li><p>Please note that there are multiple sources of papers on MDPs and RL</p>
</li>
<li><p>It is recommended that each team should choose three to four papers of your interest from the options given below and discuss with the instructor, and then fix a paper for the project</p>
</li>
<li><p><a href="https://github.com/junhyukoh/deep-reinforcement-learning-papers">Deep Reinforcement Learning Papers</a></p>
</li>
<li><p><a href="https://github.com/aikorea/awesome-rl">Awesome Reinforcement Learning (Papers and Theses)</a></p>
</li>
<li><p><a href="https://vmayoral.github.io/robots,/ai,/deep/learning,/rl,/reinforcement/learning/2016/07/06/rl-intro/">Reinforcement Learning in Robotics</a></p>
</li>
<li><p><a href="http://web.stanford.edu/class/cs234/project.html">Stanford CS 234 project ideas</a></p>
</li>
<li><p><a href="https://www.cs.mcgill.ca/~jpineau/ICLR2019-ReproducibilityChallenge.html">Reproducibility Challenge (look at RL options)</a></p>
</li>
<li><p><a href="https://web.stanford.edu/class/msande338/final_project.html">MSE338 project ideas</a></p>
</li>
<li><p><a href="https://github.com/txizzle/reading-list">Find out RL or MDP papers</a></p>
</li>
<li><p><a href="https://www.cs.utexas.edu/~eladlieb/RLRG.html">Reading group on RL - look at options towards the end</a></p>
</li>
<li><p><a href="http://grla.wikidot.com/frl">Reading group on RL - look at options towards the end</a></p>
</li>
<li><p><a href="https://spinningup.openai.com/en/latest/spinningup/keypapers.html">Keypapers in Deep RL</a></p>
</li>
<li><p><a href="https://www.reddit.com/r/MachineLearning/comments/6jvhna/d_recommendation_for_must_read_rl_and_deeprl/">Reddit recommendations</a></p>
</li>
<li><p><a href="mailto:https://medium.com/@jianzhang_23841/a-comprehensive-summary-and-categorization-on-reinforcement-learning-papers-at-icml-2018-787f899b14cb">RL papers</a></p>
</li>
<li><p><a href="https://skymind.ai/wiki/deep-reinforcement-learning">Look at the references towards the end of the page</a></p>
</li>
</ul>
</div></div>
<h2>Research paper options - Bandits</h2>
<div class="infoblock">
<div class="blockcontent">
<ul>
<li><p>This is another collection of papers and webpages which can be used for presentations related to Bandits</p>
</li>
<li><p><a href="https://docs.google.com/document/d/e/2PACX-1vQmsr1eO29D65TDIBueq4TW_D47tQyCxvCkBUwYzcClbBuA1dqXEYdUWud9eVqvWM17aF2DZVjF3C2R/pub">Papers on Bandits</a></p>
</li>
<li><p><a href="http://www.cse.iitm.ac.in/~prashla/cs6046.html">IIT-M course on Bandits</a></p>
</li>
<li><p><a href="http://banditalgs.com/">A blog on algorithm for bandits</a></p>
</li>
<li><p><a href="https://cseweb.ucsd.edu/~kamalika/teaching/CSE291W11/index.html">Another course</a></p>
</li>
<li><p><a href="http://imagine.enpc.fr/~audibert/Bandit_small.pdf">Tutorial</a></p>
</li>
<li><p><a href="https://sites.google.com/site/banditstutorial/">Tutorial</a></p>
</li>
</ul>
</div></div>
<h2>Options for software</h2>
<div class="infoblock">
<div class="blockcontent">
<ul>
<li><p>Please pick a software and discuss with the instructor</p>
</li>
<li><p>Software for RL in robotics - Tutorial on Gazebo, ROS, and OpenAI Gym</p>
</li>
<li><p>NS3 (network simulator 3) and link between NS3 and OpenAI Gym for RL in communication network applications</p>
</li>
<li><p>Tensorflow for Deep RL</p>
</li>
<li><p><a href="https://github.com/keras-rl/keras-rl">RL using Keras</a></p>
</li>
<li><p><a href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html">RL using PyTorch</a></p>
</li>
</ul>
</div></div>
<div id="footer">
<div id="footer-text">
Page generated 2019-04-13 07:09:48 IST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
